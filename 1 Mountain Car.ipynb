강화 학습이란?81
강화학습은 기계 학습의 한 영역으로 여러 시행착오를 거치며 경험을 통해 행동방식을 배워나가는 과정이다.
특정 행동을 취할 때마다 외부환경에서는 보상을 주는데, 이 보상이 강화학습에서 중요한 요소중 하나이다.
"![Gang wha](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/4916/12365.jpeg)\n",
또한 강화 학습의 또 다른 중요한 요소는 끊임없이 변화하는 환경의 상태이다. 취하는 행동은 여러분 주변의 환경을 변화시킵니다. 여러분은 이 끊임없이 변화하는 환경 속에서 계속 보상을 최대한 취할 수 있도록 노력해야합니다. 이 과정이 바로 강화 학습의 과정과 많이 닮아있습니다.


강화 학습 실생활

사실 강화 학습은 우리 주변에서 흔히 접할 수 있는 대부분의 문제를 다 풀 수 있는 아주 유용한 기술이다. 
로봇을 조종하는 것부터 시작해서, 자율 주행, 주식 투자, 심지어는 최근에 각광받는 ChatGPT 또한 강화 학습 기술이 적용되었다. 

      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3e7d7NMlQPE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U ray[rllib]\n",
        "!pip install gputil\n",
        "!pip install ipython==7.10.0\n",
        "!pip install moviepy\n",
        "!pip install gymnasium[classic-control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk7kE26_lTjF"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import ray\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "\n",
        "def view_video(filename):\n",
        "    video = f'video/{filename}-episode-0.mp4'\n",
        "    mp4 = open(video,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ]
    },
문제 풀어보기
강화학습에서 가장 단순하고 유명한 예제 중 하나인 "Mountain Car" 를 소개할 것 이다.
이  문제를 간략하게 설명하면 당신의 목표는 산 정상의 깃발에 도달하는 것이다 또 자동차는 오른쪽, 왼쪽으로 가속하거나 페달을 밟지 않을 수 있다.
하지만 산의 정상까지 가파르기 때문에, 중력 때문에 못 올라갈 수도 있으므로 적절하게 페달을 밟아야 한다.
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X25O_F-BlbXG"
      },
      "outputs": [],
      "source": [
        "env = gym.make('MountainCar-v0')\n",
        "print('Created env:', env)\n",
        "print('Available actions:', env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6uZsoNTMAI3"
      },
      "source": [
        "다음으로, 이 환경에서 받을 수 있는 상태에 대한 정보는 $s_{t} = (x_{t}, v_{t})$입니다.\n",
        "즉, 여러분이 받는 상태 정보 $s_{t}$는 $x$축 방향에서의 현재 위치 $x_{t}$와 속도 $v_{t}$입니다.\n",
        "\n",
        "$x$축 방향의 좌표는 $(-1.2, 0.6)$ 사이이고, 속도 $v$는 (-0.07, 0.07) 사이입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XWStLA-ldOd"
      },
      "outputs": [],
      "source": [
        "state, _ = env.reset()\n",
        "print('Observation space:', env.observation_space)\n",
        "print('The starting state is:', state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iOZ19FJMAI3"
      },
      "source": [
        "여러분이 어떤 행동을 선택하면, 다음과 같이 환경의 상태가 변화합니다.\n",
        "\n",
        "$v_{t+1} = v_{t} + (\\mathrm{action} - 1) * F - \\cos(3 * x_{t}) * mg$\n",
        "\n",
        "$x_{t+1} = x_{t} + v_{t+1}$\n",
        "\n",
        "여기서 $F$는 페달을 밟을 때 가속하는 힘 $F = 0.001$이고, 중력 $mg = 0.0025$입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKYtgu-blgGL"
      },
      "outputs": [],
      "source": [
        "action = 0\n",
        "state, reward, done, _, _ = env.step(action)\n",
        "print(f\"State: {state}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Done: {done}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCUIOuWYq0Nq"
      },
      "source": [
        "### 인공지능 정책 정의하기\n",
        "\n",
        "강화 학습에서 정책은 인공지능의 두뇌입니다. 인공지능이 변화하는 환경 속에서 어떻게 행동할지를 결정하는 지침으로 생각할 수 있습니다. 가장 단순한 형태의 정책으로는 여러분이 취할 수 있는 행동 중에서 아무거나 (샘플링하여) 고르는 것이 있겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o18p51sq0lv"
      },
      "outputs": [],
      "source": [
        "class RandomPolicy():\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def __call__(self, state):\n",
        "        return self.action_space.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvVYPQrq3h_"
      },
      "source": [
        "### 환경과 상호작용하기\n",
        "\n",
        "정책이 있다면, 여러분은 주어진 상황에서 어떤 행동을 취해야 할지 결정할 수 있습니다. 그럼 이제 본격적으로 환경 속에 들어가서 직접 부딪혀볼까요?\n",
        "\n",
        "여러분이 어떤 행동을 취하면, 환경은 여러분에게 보상을 줍니다. 이 보상은 여러분이 좋은 행동을 했는지, 나쁜 행동을 했는지 알려주는 값입니다. 그리고 여러분이 취한 행동에 따라 환경은 변화하고, 이 변화한 환경을 여러분은 다시 관찰할 수 있습니다. 그리고 여러분은 또다시 (여러분이 관찰한 결과를 토대로) 어떤 행동을 취할지 결정합니다. 강화 학습은 이 과정을 반복하며 진행됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwtH_tezxFYm"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, 'video', name_prefix='random-agent')\n",
        "\n",
        "agent = RandomPolicy(env.action_space)\n",
        "state, _ = env.reset()\n",
        "env.start_video_recorder()\n",
        "\n",
        "for t in range(100):\n",
        "    action = agent(state)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "view_video('random-agent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AY6GrOpMAI5"
      },
      "source": [
        "### BONUS: 강화 학습 에이전트 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxmeXv3xlrgD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "algo = (\n",
        "    PPOConfig()\n",
        "    .rollouts(num_rollout_workers=1)\n",
        "    .resources(num_gpus=1)\n",
        "    .environment(env=\"MountainCar-v0\")\n",
        "    .build()\n",
        ")\n",
        "\n",
        "for i in range(2):\n",
        "    algo.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOpScLYmMAI5"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, 'video', name_prefix='ppo-agent')\n",
        "\n",
        "state, _ = env.reset()\n",
        "env.start_video_recorder()\n",
        "\n",
        "for t in range(100):\n",
        "    action = algo.compute_single_action(state)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "view_video('ppo-agent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptnw16PQMAI5"
      },
      "source": [
        "# 강화 학습 환경 직접 만져보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBtA3hHnMAI5"
      },
      "source": [
        "### 블랙잭"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3QTd9SGMAI6"
      },
      "outputs": [],
      "source": [
        "# Create the Blackjack environment\n",
        "env = gym.make('Blackjack-v0')\n",
        "\n",
        "# Reset the environment\n",
        "state = env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    # Choose a random action\n",
        "    action = ???\n",
        "\n",
        "    # Take a step in the environment\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Print the current state, reward, and whether the episode is done\n",
        "    print('State:', state)\n",
        "    print('Reward:', reward)\n",
        "    print('Done:', done)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqFW8kfkMAI6"
      },
      "source": [
        "### Tic-Tac-Toe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIcXpI4WMAI6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class TicTacToeEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "        self.action_space = spaces.Discrete(9)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 3), dtype=int)\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "        return self.board\n",
        "\n",
        "    def step(self, action):\n",
        "        row = action // 3\n",
        "        col = action % 3\n",
        "\n",
        "        if self.board[row, col] != 0:\n",
        "            return self.board, -1, False, False, {}\n",
        "\n",
        "        self.board[row, col] = self.current_player\n",
        "\n",
        "        if self._check_winner(self.current_player):\n",
        "            return self.board, 1, True, False, {}\n",
        "\n",
        "        if np.count_nonzero(self.board) == 9:\n",
        "            return self.board, 0, True, False, {}\n",
        "\n",
        "        self.current_player = -self.current_player\n",
        "        return self.board, 0, False, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(self.board)\n",
        "\n",
        "    def _check_winner(self, player):\n",
        "        for i in range(3):\n",
        "            if np.all(self.board[i, :] == player) or np.all(self.board[:, i] == player):\n",
        "                return True\n",
        "        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1AE-wOrMAI6"
      },
      "outputs": [],
      "source": [
        "env = TicTacToeEnv()\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = ?????\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
